{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303217, 6)\n",
      "(101073, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394617</th>\n",
       "      <td>394617</td>\n",
       "      <td>527528</td>\n",
       "      <td>527529</td>\n",
       "      <td>How can I become a CEO if my law school grades...</td>\n",
       "      <td>What's a good online discussion board where I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146735</th>\n",
       "      <td>146735</td>\n",
       "      <td>60120</td>\n",
       "      <td>231728</td>\n",
       "      <td>Can a Singapore citizen obtain another citizen...</td>\n",
       "      <td>As an American, could I cross the Canadian bor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231076</th>\n",
       "      <td>231076</td>\n",
       "      <td>340783</td>\n",
       "      <td>167045</td>\n",
       "      <td>How can you get rid of pimples in your earlobe?</td>\n",
       "      <td>How do you get rid of a pimple in your ear?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66117</th>\n",
       "      <td>66117</td>\n",
       "      <td>103992</td>\n",
       "      <td>26685</td>\n",
       "      <td>How will releasing new 500 and 2000 rupee note...</td>\n",
       "      <td>If PM Modi wants to curb black money? Why was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114046</th>\n",
       "      <td>114046</td>\n",
       "      <td>8431</td>\n",
       "      <td>89956</td>\n",
       "      <td>What are some possible solutions if I forgot m...</td>\n",
       "      <td>How do I get my iCloud password?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "394617  394617  527528  527529   \n",
       "146735  146735   60120  231728   \n",
       "231076  231076  340783  167045   \n",
       "66117    66117  103992   26685   \n",
       "114046  114046    8431   89956   \n",
       "\n",
       "                                                question1  \\\n",
       "394617  How can I become a CEO if my law school grades...   \n",
       "146735  Can a Singapore citizen obtain another citizen...   \n",
       "231076    How can you get rid of pimples in your earlobe?   \n",
       "66117   How will releasing new 500 and 2000 rupee note...   \n",
       "114046  What are some possible solutions if I forgot m...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "394617  What's a good online discussion board where I ...             0  \n",
       "146735  As an American, could I cross the Canadian bor...             0  \n",
       "231076        How do you get rid of a pimple in your ear?             1  \n",
       "66117   If PM Modi wants to curb black money? Why was ...             1  \n",
       "114046                   How do I get my iCloud password?             1  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    191290\n",
       "1    111927\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What portion of our questions are actually duplicates?\n",
    "df_train['is_duplicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       1\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are we missing any data?\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0.0\n",
       "qid1            0.0\n",
       "qid2            0.0\n",
       "question1       0.0\n",
       "question2       0.0\n",
       "is_duplicate    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there any duplicate rows?\n",
    "df_train[df_train.duplicated()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 303217 entries, 394617 to 128037\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   id            303217 non-null  int64 \n",
      " 1   qid1          303217 non-null  int64 \n",
      " 2   qid2          303217 non-null  int64 \n",
      " 3   question1     303217 non-null  object\n",
      " 4   question2     303216 non-null  object\n",
      " 5   is_duplicate  303217 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 16.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I become a CEO if my law school grades...</td>\n",
       "      <td>What's a good online discussion board where I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can a Singapore citizen obtain another citizen...</td>\n",
       "      <td>As an American, could I cross the Canadian bor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you get rid of pimples in your earlobe?</td>\n",
       "      <td>How do you get rid of a pimple in your ear?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How will releasing new 500 and 2000 rupee note...</td>\n",
       "      <td>If PM Modi wants to curb black money? Why was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some possible solutions if I forgot m...</td>\n",
       "      <td>How do I get my iCloud password?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  How can I become a CEO if my law school grades...   \n",
       "1  Can a Singapore citizen obtain another citizen...   \n",
       "2    How can you get rid of pimples in your earlobe?   \n",
       "3  How will releasing new 500 and 2000 rupee note...   \n",
       "4  What are some possible solutions if I forgot m...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What's a good online discussion board where I ...             0  \n",
       "1  As an American, could I cross the Canadian bor...             0  \n",
       "2        How do you get rid of a pimple in your ear?             1  \n",
       "3  If PM Modi wants to curb black money? Why was ...             1  \n",
       "4                   How do I get my iCloud password?             1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.loc[:,'question1':'is_duplicate'].reset_index(drop=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def clean_all(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove stopwords  \n",
    "    eng_stopwords = stopwords.words('English')\n",
    "    text = [word for word in text.split() if word not in eng_stopwords]\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "\n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    text = ''.join([ps.stem(word) for word in text])       \n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# # Create a Transformer from the function so that we can use it in a Pipeline\n",
    "# cleaner = FunctionTransformer(clean_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I become a CEO if my law school grades...</td>\n",
       "      <td>What's a good online discussion board where I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can a Singapore citizen obtain another citizen...</td>\n",
       "      <td>As an American, could I cross the Canadian bor...</td>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you get rid of pimples in your earlobe?</td>\n",
       "      <td>How do you get rid of a pimple in your ear?</td>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How will releasing new 500 and 2000 rupee note...</td>\n",
       "      <td>If PM Modi wants to curb black money? Why was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some possible solutions if I forgot m...</td>\n",
       "      <td>How do I get my iCloud password?</td>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Are there any languages that use the same word...</td>\n",
       "      <td>Is it widespread for languages to use the same...</td>\n",
       "      <td>0</td>\n",
       "      <td>language use word iron steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My crush didn't accept my friend request, but ...</td>\n",
       "      <td>If my crush has not accepted my friend request...</td>\n",
       "      <td>0</td>\n",
       "      <td>crush didnt accept friend request accepted req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Is Sun in bal awastha i.e at 0 degree in 9th h...</td>\n",
       "      <td>Is Sun in bal awastha i.e at 0 degree in 9th h...</td>\n",
       "      <td>1</td>\n",
       "      <td>sun bal awastha ie 0 degree 9th house sign leo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where can I buy Nestle Wonder Balls?</td>\n",
       "      <td>How good is Nestle Pure Life Water for you?</td>\n",
       "      <td>0</td>\n",
       "      <td>buy nestle wonder ball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I backup my pictures and music from my ...</td>\n",
       "      <td>How do I transfer music from iTunes to iPhone?</td>\n",
       "      <td>0</td>\n",
       "      <td>backup picture music iphone itunes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  How can I become a CEO if my law school grades...   \n",
       "1  Can a Singapore citizen obtain another citizen...   \n",
       "2    How can you get rid of pimples in your earlobe?   \n",
       "3  How will releasing new 500 and 2000 rupee note...   \n",
       "4  What are some possible solutions if I forgot m...   \n",
       "5  Are there any languages that use the same word...   \n",
       "6  My crush didn't accept my friend request, but ...   \n",
       "7  Is Sun in bal awastha i.e at 0 degree in 9th h...   \n",
       "8               Where can I buy Nestle Wonder Balls?   \n",
       "9  How do I backup my pictures and music from my ...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What's a good online discussion board where I ...             0   \n",
       "1  As an American, could I cross the Canadian bor...             0   \n",
       "2        How do you get rid of a pimple in your ear?             1   \n",
       "3  If PM Modi wants to curb black money? Why was ...             1   \n",
       "4                   How do I get my iCloud password?             1   \n",
       "5  Is it widespread for languages to use the same...             0   \n",
       "6  If my crush has not accepted my friend request...             0   \n",
       "7  Is Sun in bal awastha i.e at 0 degree in 9th h...             1   \n",
       "8        How good is Nestle Pure Life Water for you?             0   \n",
       "9     How do I transfer music from iTunes to iPhone?             0   \n",
       "\n",
       "                                   question1_cleaned  \n",
       "0            become ceo law school grade competitive  \n",
       "1  singapore citizen obtain another citizenship b...  \n",
       "2                             get rid pimple earlobe  \n",
       "3  releasing new 500 2000 rupee note help eradica...  \n",
       "4           possible solution forgot icloud password  \n",
       "5                       language use word iron steel  \n",
       "6  crush didnt accept friend request accepted req...  \n",
       "7  sun bal awastha ie 0 degree 9th house sign leo...  \n",
       "8                             buy nestle wonder ball  \n",
       "9                 backup picture music iphone itunes  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question1_cleaned'] = df_train['question1'].apply(lambda x: clean_all(x))\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I become a CEO if my law school grades...</td>\n",
       "      <td>What's a good online discussion board where I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can a Singapore citizen obtain another citizen...</td>\n",
       "      <td>As an American, could I cross the Canadian bor...</td>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you get rid of pimples in your earlobe?</td>\n",
       "      <td>How do you get rid of a pimple in your ear?</td>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How will releasing new 500 and 2000 rupee note...</td>\n",
       "      <td>If PM Modi wants to curb black money? Why was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some possible solutions if I forgot m...</td>\n",
       "      <td>How do I get my iCloud password?</td>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  How can I become a CEO if my law school grades...   \n",
       "1  Can a Singapore citizen obtain another citizen...   \n",
       "2    How can you get rid of pimples in your earlobe?   \n",
       "3  How will releasing new 500 and 2000 rupee note...   \n",
       "4  What are some possible solutions if I forgot m...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What's a good online discussion board where I ...             0   \n",
       "1  As an American, could I cross the Canadian bor...             0   \n",
       "2        How do you get rid of a pimple in your ear?             1   \n",
       "3  If PM Modi wants to curb black money? Why was ...             1   \n",
       "4                   How do I get my iCloud password?             1   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0            become ceo law school grade competitive   \n",
       "1  singapore citizen obtain another citizenship b...   \n",
       "2                             get rid pimple earlobe   \n",
       "3  releasing new 500 2000 rupee note help eradica...   \n",
       "4           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \n",
       "0     [become, ceo, law, school, grade, competitive]  \n",
       "1  [singapore, citizen, obtain, another, citizens...  \n",
       "2                        [get, rid, pimple, earlobe]  \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...  \n",
       "4     [possible, solution, forgot, icloud, password]  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question1_cleaned_tokenized'] = df_train['question1_cleaned'].apply(lambda x: tokenize(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I become a CEO if my law school grades...</td>\n",
       "      <td>What's a good online discussion board where I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can a Singapore citizen obtain another citizen...</td>\n",
       "      <td>As an American, could I cross the Canadian bor...</td>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can you get rid of pimples in your earlobe?</td>\n",
       "      <td>How do you get rid of a pimple in your ear?</td>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How will releasing new 500 and 2000 rupee note...</td>\n",
       "      <td>If PM Modi wants to curb black money? Why was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some possible solutions if I forgot m...</td>\n",
       "      <td>How do I get my iCloud password?</td>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Are there any languages that use the same word...</td>\n",
       "      <td>Is it widespread for languages to use the same...</td>\n",
       "      <td>0</td>\n",
       "      <td>language use word iron steel</td>\n",
       "      <td>[language, use, word, iron, steel]</td>\n",
       "      <td>widespread language use word one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My crush didn't accept my friend request, but ...</td>\n",
       "      <td>If my crush has not accepted my friend request...</td>\n",
       "      <td>0</td>\n",
       "      <td>crush didnt accept friend request accepted req...</td>\n",
       "      <td>[crush, didnt, accept, friend, request, accept...</td>\n",
       "      <td>crush accepted friend request facebook mean do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Is Sun in bal awastha i.e at 0 degree in 9th h...</td>\n",
       "      <td>Is Sun in bal awastha i.e at 0 degree in 9th h...</td>\n",
       "      <td>1</td>\n",
       "      <td>sun bal awastha ie 0 degree 9th house sign leo...</td>\n",
       "      <td>[sun, bal, awastha, ie, 0, degree, 9th, house,...</td>\n",
       "      <td>sun bal awastha ie 0 degree 9th house sign leo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where can I buy Nestle Wonder Balls?</td>\n",
       "      <td>How good is Nestle Pure Life Water for you?</td>\n",
       "      <td>0</td>\n",
       "      <td>buy nestle wonder ball</td>\n",
       "      <td>[buy, nestle, wonder, ball]</td>\n",
       "      <td>good nestle pure life water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I backup my pictures and music from my ...</td>\n",
       "      <td>How do I transfer music from iTunes to iPhone?</td>\n",
       "      <td>0</td>\n",
       "      <td>backup picture music iphone itunes</td>\n",
       "      <td>[backup, picture, music, iphone, itunes]</td>\n",
       "      <td>transfer music itunes iphone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  How can I become a CEO if my law school grades...   \n",
       "1  Can a Singapore citizen obtain another citizen...   \n",
       "2    How can you get rid of pimples in your earlobe?   \n",
       "3  How will releasing new 500 and 2000 rupee note...   \n",
       "4  What are some possible solutions if I forgot m...   \n",
       "5  Are there any languages that use the same word...   \n",
       "6  My crush didn't accept my friend request, but ...   \n",
       "7  Is Sun in bal awastha i.e at 0 degree in 9th h...   \n",
       "8               Where can I buy Nestle Wonder Balls?   \n",
       "9  How do I backup my pictures and music from my ...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What's a good online discussion board where I ...             0   \n",
       "1  As an American, could I cross the Canadian bor...             0   \n",
       "2        How do you get rid of a pimple in your ear?             1   \n",
       "3  If PM Modi wants to curb black money? Why was ...             1   \n",
       "4                   How do I get my iCloud password?             1   \n",
       "5  Is it widespread for languages to use the same...             0   \n",
       "6  If my crush has not accepted my friend request...             0   \n",
       "7  Is Sun in bal awastha i.e at 0 degree in 9th h...             1   \n",
       "8        How good is Nestle Pure Life Water for you?             0   \n",
       "9     How do I transfer music from iTunes to iPhone?             0   \n",
       "\n",
       "                                   question1_cleaned  \\\n",
       "0            become ceo law school grade competitive   \n",
       "1  singapore citizen obtain another citizenship b...   \n",
       "2                             get rid pimple earlobe   \n",
       "3  releasing new 500 2000 rupee note help eradica...   \n",
       "4           possible solution forgot icloud password   \n",
       "5                       language use word iron steel   \n",
       "6  crush didnt accept friend request accepted req...   \n",
       "7  sun bal awastha ie 0 degree 9th house sign leo...   \n",
       "8                             buy nestle wonder ball   \n",
       "9                 backup picture music iphone itunes   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "5                 [language, use, word, iron, steel]   \n",
       "6  [crush, didnt, accept, friend, request, accept...   \n",
       "7  [sun, bal, awastha, ie, 0, degree, 9th, house,...   \n",
       "8                        [buy, nestle, wonder, ball]   \n",
       "9           [backup, picture, music, iphone, itunes]   \n",
       "\n",
       "                                   question2_cleaned  \n",
       "0  whats good online discussion board air daily f...  \n",
       "1  american could cross canadian border child chi...  \n",
       "2                                 get rid pimple ear  \n",
       "3  pm modi want curb black money new 2000 rupee n...  \n",
       "4                                get icloud password  \n",
       "5                   widespread language use word one  \n",
       "6  crush accepted friend request facebook mean do...  \n",
       "7  sun bal awastha ie 0 degree 9th house sign leo...  \n",
       "8                        good nestle pure life water  \n",
       "9                       transfer music itunes iphone  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question2_cleaned'] = df_train['question2'].apply(lambda x: clean_all(x))\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  \n",
       "0  [whats, good, online, discussion, board, air, ...  \n",
       "1  [american, could, cross, canadian, border, chi...  \n",
       "2                            [get, rid, pimple, ear]  \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...  \n",
       "4                            [get, icloud, password]  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question2_cleaned_tokenized'] = df_train['question2_cleaned'].apply(lambda x: tokenize(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[:,'is_duplicate':'question2_cleaned_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  \n",
       "0  [whats, good, online, discussion, board, air, ...  \n",
       "1  [american, could, cross, canadian, border, chi...  \n",
       "2                            [get, rid, pimple, ear]  \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...  \n",
       "4                            [get, icloud, password]  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle.dump(df_train, open(\"df_train.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load( open( \"df_train.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  \n",
       "0  [whats, good, online, discussion, board, air, ...  \n",
       "1  [american, could, cross, canadian, border, chi...  \n",
       "2                            [get, rid, pimple, ear]  \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...  \n",
       "4                            [get, icloud, password]  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1 - Cosine Similarity of TfidfVectorized Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "Document1 = df_train['question1_cleaned']\n",
    "Document2 = df_train['question2_cleaned']\n",
    "\n",
    "corpus = pd.concat([Document1, Document2])\n",
    "\n",
    "corpus\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "vec_question1_train = vectorizer.transform(Document1)\n",
    "vec_question2_train = vectorizer.transform(Document2)\n",
    "\n",
    "cos_sim_lst = []\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "       \n",
    "        cos_sim_lst.append(cosine_similarity(vec_question1_train[i], vec_question2_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(cos_sim_lst, open(\"cos_sim_lst_tfidf.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_lst = pickle.load( open( \"cos_sim_lst_tfidf.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_list_new = []\n",
    "\n",
    "for i in range(len(cos_sim_lst)):\n",
    "    \n",
    "    lst = cos_sim_lst[i][0].tolist()\n",
    "    cos_sim_list_new.append(lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tfidf_cos_sim'] = cos_sim_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "      <th>tfidf_cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "      <td>0.200881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "      <td>0.584816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "      <td>0.415588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "      <td>0.628217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  tfidf_cos_sim  \n",
       "0  [whats, good, online, discussion, board, air, ...       0.000000  \n",
       "1  [american, could, cross, canadian, border, chi...       0.200881  \n",
       "2                            [get, rid, pimple, ear]       0.584816  \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...       0.415588  \n",
       "4                            [get, icloud, password]       0.628217  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_train, open(\"df_train_feature1.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.00%\n",
      "Test set accuracy: 66.68%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_train = accuracy_score(y_pred_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Training set accuracy: {:.2f}%\".format(accuracy_train * 100))\n",
    "print(\"Test set accuracy: {:.2f}%\".format(accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.00%\n",
      "Test set accuracy: 67.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_train = accuracy_score(y_pred_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Training set accuracy: {:.2f}%\".format(accuracy_train * 100))\n",
    "print(\"Test set accuracy: {:.2f}%\".format(accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.00%\n",
      "Test set accuracy: 67.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(max_depth=2)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_train = accuracy_score(y_pred_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Training set accuracy: {:.2f}%\".format(accuracy_train * 100))\n",
    "print(\"Test set accuracy: {:.2f}%\".format(accuracy_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 2 - Count of Words Common to both Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load( open( \"df_train_feature1.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "      <th>tfidf_cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "      <td>0.200881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "      <td>0.584816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "      <td>0.415588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "      <td>0.628217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  tfidf_cos_sim  \n",
       "0  [whats, good, online, discussion, board, air, ...       0.000000  \n",
       "1  [american, could, cross, canadian, border, chi...       0.200881  \n",
       "2                            [get, rid, pimple, ear]       0.584816  \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...       0.415588  \n",
       "4                            [get, icloud, password]       0.628217  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used code from here: https://stackoverflow.com/questions/49796271/find-intersection-of-two-columns-in-python-pandas-list-of-strings\n",
    "\n",
    "df_train['common_words'] = [len(set(a).intersection(b)) for a, b in zip(df_train['question1_cleaned_tokenized'], df_train['question2_cleaned_tokenized'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "      <th>tfidf_cos_sim</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "      <td>0.200881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "      <td>0.415588</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "      <td>0.628217</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  tfidf_cos_sim  \\\n",
       "0  [whats, good, online, discussion, board, air, ...       0.000000   \n",
       "1  [american, could, cross, canadian, border, chi...       0.200881   \n",
       "2                            [get, rid, pimple, ear]       0.584816   \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...       0.415588   \n",
       "4                            [get, icloud, password]       0.628217   \n",
       "\n",
       "   common_words  \n",
       "0             0  \n",
       "1             2  \n",
       "2             3  \n",
       "3             6  \n",
       "4             2  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_train, open(\"df_train_feature2.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.6681232788615339\n",
      "Test accuracy:\t0.6679803443044654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline), \n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim','common_words']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.6953976551291987\n",
      "Test accuracy:\t0.6924345359804762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline), \n",
    "    ('model', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim','common_words']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.6726209125537984\n",
      "Test accuracy:\t0.6727293714134952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline), \n",
    "    ('model', RandomForestClassifier(max_depth=2))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim','common_words']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3 - Count of Words in Each Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickle.load( open( \"df_train_feature2.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "      <th>tfidf_cos_sim</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "      <td>0.200881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "      <td>0.415588</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "      <td>0.628217</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  tfidf_cos_sim  \\\n",
       "0  [whats, good, online, discussion, board, air, ...       0.000000   \n",
       "1  [american, could, cross, canadian, border, chi...       0.200881   \n",
       "2                            [get, rid, pimple, ear]       0.584816   \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...       0.415588   \n",
       "4                            [get, icloud, password]       0.628217   \n",
       "\n",
       "   common_words  \n",
       "0             0  \n",
       "1             2  \n",
       "2             3  \n",
       "3             6  \n",
       "4             2  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "      <th>tfidf_cos_sim</th>\n",
       "      <th>common_words</th>\n",
       "      <th>question1_word_count</th>\n",
       "      <th>question2_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "      <td>0.200881</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "      <td>0.415588</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "      <td>0.628217</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question2_cleaned_tokenized  tfidf_cos_sim  \\\n",
       "0  [whats, good, online, discussion, board, air, ...       0.000000   \n",
       "1  [american, could, cross, canadian, border, chi...       0.200881   \n",
       "2                            [get, rid, pimple, ear]       0.584816   \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...       0.415588   \n",
       "4                            [get, icloud, password]       0.628217   \n",
       "\n",
       "   common_words  question1_word_count  question2_word_count  \n",
       "0             0                     6                     8  \n",
       "1             2                    12                     9  \n",
       "2             3                     4                     4  \n",
       "3             6                    10                    11  \n",
       "4             2                     5                     3  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question1_word_count'] = df_train['question1_cleaned_tokenized'].apply(lambda x: len(x))\n",
    "df_train['question2_word_count'] = df_train['question2_cleaned_tokenized'].apply(lambda x: len(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(df_train, open(\"df_train_feature3.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.6661527299111192\n",
      "Test accuracy:\t0.6662159488160412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline), \n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim','common_words','question1_word_count','question2_word_count']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.6977804528140099\n",
      "Test accuracy:\t0.6973979288965108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline), \n",
    "    ('model', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim','common_words','question1_word_count','question2_word_count']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.6744842768332702\n",
      "Test accuracy:\t0.6741474836752193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline), \n",
    "    ('model', RandomForestClassifier(max_depth=2))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['tfidf_cos_sim','common_words','question1_word_count','question2_word_count']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3 - Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop rows with missing values\n",
    "# df = df.dropna(axis=0)\n",
    "\n",
    "# # drop duplicate rows\n",
    "# df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import BernoulliNB # Bernoulli because we have binary features\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing_pipeline = Pipeline(steps=[\n",
    "#     ('cleaning',cleaner),\n",
    "#     ('preprocessing',CountVectorizer())\n",
    "# ])\n",
    "\n",
    "# preprocessing = ColumnTransformer(transformers=[\n",
    "#     ('preprocessing_1', preprocessing_pipeline,'question1'),\n",
    "#     ('preprocessing_2', preprocessing_pipeline,'question2')\n",
    "# ])\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     # ('cleaning', cleaning),\n",
    "#     ('preprocessing', preprocessing), \n",
    "#     ('model', BernoulliNB())\n",
    "# ])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_train[['question1','question2']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# train_accuracy = pipeline.score(X_train, y_train)\n",
    "# test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "# print(f'Train accuracy:\\t{train_accuracy}')\n",
    "# print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer & BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.7592220041884471\n",
      "Test accuracy:\t0.724853241870589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB # Bernoulli because we have binary features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=\n",
    "[\n",
    "    ('preprocessing_1', CountVectorizer() ,'question1_cleaned'),\n",
    "    ('preprocessing_2', CountVectorizer() ,'question2_cleaned')])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing), \n",
    "    ('model', BernoulliNB())\n",
    "    ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['question1_cleaned','question2_cleaned']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer & BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.7592220041884471\n",
      "Test accuracy:\t0.724853241870589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB # Bernoulli because we have binary features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=\n",
    "[\n",
    "    ('preprocessing_1', TfidfVectorizer() ,'question1_cleaned'),\n",
    "    ('preprocessing_2', TfidfVectorizer() ,'question2_cleaned')])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing), \n",
    "    ('model', BernoulliNB())\n",
    "    ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['question1_cleaned','question2_cleaned']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec & BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleaned</th>\n",
       "      <th>question2_cleaned</th>\n",
       "      <th>question1_cleaned_tokenized</th>\n",
       "      <th>question2_cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>become ceo law school grade competitive</td>\n",
       "      <td>whats good online discussion board air daily f...</td>\n",
       "      <td>[become, ceo, law, school, grade, competitive]</td>\n",
       "      <td>[whats, good, online, discussion, board, air, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>singapore citizen obtain another citizenship b...</td>\n",
       "      <td>american could cross canadian border child chi...</td>\n",
       "      <td>[singapore, citizen, obtain, another, citizens...</td>\n",
       "      <td>[american, could, cross, canadian, border, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>get rid pimple earlobe</td>\n",
       "      <td>get rid pimple ear</td>\n",
       "      <td>[get, rid, pimple, earlobe]</td>\n",
       "      <td>[get, rid, pimple, ear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>releasing new 500 2000 rupee note help eradica...</td>\n",
       "      <td>pm modi want curb black money new 2000 rupee n...</td>\n",
       "      <td>[releasing, new, 500, 2000, rupee, note, help,...</td>\n",
       "      <td>[pm, modi, want, curb, black, money, new, 2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>possible solution forgot icloud password</td>\n",
       "      <td>get icloud password</td>\n",
       "      <td>[possible, solution, forgot, icloud, password]</td>\n",
       "      <td>[get, icloud, password]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                  question1_cleaned  \\\n",
       "0             0            become ceo law school grade competitive   \n",
       "1             0  singapore citizen obtain another citizenship b...   \n",
       "2             1                             get rid pimple earlobe   \n",
       "3             1  releasing new 500 2000 rupee note help eradica...   \n",
       "4             1           possible solution forgot icloud password   \n",
       "\n",
       "                                   question2_cleaned  \\\n",
       "0  whats good online discussion board air daily f...   \n",
       "1  american could cross canadian border child chi...   \n",
       "2                                 get rid pimple ear   \n",
       "3  pm modi want curb black money new 2000 rupee n...   \n",
       "4                                get icloud password   \n",
       "\n",
       "                         question1_cleaned_tokenized  \\\n",
       "0     [become, ceo, law, school, grade, competitive]   \n",
       "1  [singapore, citizen, obtain, another, citizens...   \n",
       "2                        [get, rid, pimple, earlobe]   \n",
       "3  [releasing, new, 500, 2000, rupee, note, help,...   \n",
       "4     [possible, solution, forgot, icloud, password]   \n",
       "\n",
       "                         question2_cleaned_tokenized  \n",
       "0  [whats, good, online, discussion, board, air, ...  \n",
       "1  [american, could, cross, canadian, border, chi...  \n",
       "2                            [get, rid, pimple, ear]  \n",
       "3  [pm, modi, want, curb, black, money, new, 2000...  \n",
       "4                            [get, icloud, password]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['question1_cleaned_tokenized'] = df_train['question1_cleaned'].apply(lambda x: tokenize(x))\n",
    "# df_train['question2_cleaned_tokenized'] = df_train['question2_cleaned'].apply(lambda x: tokenize(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.train of <gensim.models.word2vec.Word2Vec object at 0x0000023BDECB6B20>>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "Model_CBoW = gensim.models.Word2Vec(df_train['question1_cleaned_tokenized'], vector_size = 100, window = 5, min_count = 1)\n",
    "Model_CBoW.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_CBoW = gensim.models.Word2Vec(df_train['question2_cleaned_tokenized'], vector_size = 100, window = 5, min_count = 1)\n",
    "Model_CBoW.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30626673, -1.4813572 ,  1.4438479 ,  0.1176673 , -0.05845909,\n",
       "       -0.6039322 ,  0.14252529, -0.0250668 ,  0.63708794, -0.06565531,\n",
       "       -0.63601655, -1.3520248 ,  1.3923845 ,  0.86414075,  0.12141857,\n",
       "        1.2088464 , -0.29210582, -0.94989145,  0.08012938, -0.38129961,\n",
       "        1.785485  , -1.2206194 ,  0.29603723, -1.674993  , -0.9170058 ,\n",
       "        1.0199337 , -0.9172853 , -1.8531643 , -1.6108813 , -0.5632631 ,\n",
       "        1.6912004 ,  0.8579326 , -2.1741757 ,  1.098585  ,  0.88569295,\n",
       "        2.89512   ,  0.2630835 , -0.8194446 ,  0.7330894 , -1.5767498 ,\n",
       "        0.66460997, -0.43954933, -3.4969757 , -1.5781193 ,  2.1128495 ,\n",
       "       -0.5824082 , -3.119341  ,  0.9600089 ,  0.38305953,  0.4144633 ,\n",
       "        0.7754835 , -1.2191876 ,  0.04605655,  0.48488665, -1.2559518 ,\n",
       "        0.02976516,  0.7597034 ,  1.8602806 ,  0.4396544 ,  1.4920886 ,\n",
       "       -0.6650546 ,  2.2185013 ,  0.99010766, -0.5134579 , -2.3844233 ,\n",
       "       -0.6341457 ,  1.2214446 ,  0.09481429, -0.24935493,  0.17028409,\n",
       "       -2.1229885 , -1.7645575 , -1.6887331 ,  0.7203114 ,  0.64106196,\n",
       "       -2.1894732 ,  0.80144733, -3.199191  , -1.4004277 ,  0.44590423,\n",
       "        0.95415455, -0.08097488, -1.9972087 ,  0.80214554, -1.4235443 ,\n",
       "       -1.8384535 ,  0.87870127,  0.54595953,  0.11158866, -0.94673914,\n",
       "        0.8605662 , -0.5296224 , -1.311694  ,  0.9130577 , -1.3220857 ,\n",
       "        0.82278633,  1.7891339 , -1.4491944 ,  0.21507575,  0.5736834 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_CBoW.wv['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('college', 0.7359702587127686),\n",
       " ('schooler', 0.7320328950881958),\n",
       " ('harvard', 0.7086325883865356),\n",
       " ('undergrad', 0.705434262752533),\n",
       " ('juilliard', 0.7047107219696045),\n",
       " ('grade', 0.7000455856323242),\n",
       " ('stanford', 0.68809974193573),\n",
       " ('mit', 0.6841893792152405),\n",
       " ('graduate', 0.6746595501899719),\n",
       " ('literay', 0.6720913648605347)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_CBoW.wv.most_similar('school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Word2Vec.train of <gensim.models.word2vec.Word2Vec object at 0x0000023B88750940>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gensim\n",
    "\n",
    "# Model_CBoW = gensim.models.Word2Vec(df_train[['question1_cleaned_tokenized','question2_cleaned_tokenized']], vector_size = 100, window = 5, min_count = 1)\n",
    "# Model_CBoW.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All estimators should implement fit and transform, or can be 'drop' or 'passthrough' specifiers. 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>' (type <class 'gensim.models.word2vec.Word2Vec'>) doesn't.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 18\u001b[0m\n\u001b[0;32m     11\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     12\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mpreprocessing\u001b[39m\u001b[39m'\u001b[39m, preprocessing), \n\u001b[0;32m     13\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, BernoulliNB())\n\u001b[0;32m     14\u001b[0m     ])\n\u001b[0;32m     16\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(df_train[[\u001b[39m'\u001b[39m\u001b[39mquestion1_cleaned\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mquestion2_cleaned\u001b[39m\u001b[39m'\u001b[39m]], df_train[\u001b[39m'\u001b[39m\u001b[39mis_duplicate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m), test_size\u001b[39m=\u001b[39m\u001b[39m0.20\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m train_accuracy \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mscore(X_train, y_train)\n\u001b[0;32m     20\u001b[0m test_accuracy \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 378\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    335\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    337\u001b[0m     cloned_transformer,\n\u001b[0;32m    338\u001b[0m     X,\n\u001b[0;32m    339\u001b[0m     y,\n\u001b[0;32m    340\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    343\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    344\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\compose\\_column_transformer.py:686\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[39m# set n_features_in_ attribute\u001b[39;00m\n\u001b[0;32m    685\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 686\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_transformers()\n\u001b[0;32m    687\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    688\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\compose\\_column_transformer.py:358\u001b[0m, in \u001b[0;36mColumnTransformer._validate_transformers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(t, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mhasattr\u001b[39m(t, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[0;32m    356\u001b[0m     t, \u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m ):\n\u001b[1;32m--> 358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAll estimators should implement fit and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransform, or can be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecifiers. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (t, \u001b[39mtype\u001b[39m(t))\n\u001b[0;32m    362\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: All estimators should implement fit and transform, or can be 'drop' or 'passthrough' specifiers. 'Word2Vec<vocab=0, vector_size=100, alpha=0.025>' (type <class 'gensim.models.word2vec.Word2Vec'>) doesn't."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB # Bernoulli because we have binary features\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# preprocessing = ColumnTransformer(transformers=\n",
    "# [\n",
    "#     ('preprocessing_1', gensim.models.Word2Vec() ,'question1_cleaned'),\n",
    "#     ('preprocessing_2', gensim.models.Word2Vec() ,'question2_cleaned')])\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessing', preprocessing), \n",
    "#     ('model', BernoulliNB())\n",
    "#     ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[['question1_cleaned','question2_cleaned']], df_train['is_duplicate'].astype('int'), test_size=0.20, random_state=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tokenization\n",
    "import nltk\n",
    "\n",
    "# For converting words into frequency counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>500</th>\n",
       "      <th>black</th>\n",
       "      <th>curb</th>\n",
       "      <th>eradicating</th>\n",
       "      <th>help</th>\n",
       "      <th>introduced</th>\n",
       "      <th>modi</th>\n",
       "      <th>money</th>\n",
       "      <th>new</th>\n",
       "      <th>note</th>\n",
       "      <th>pm</th>\n",
       "      <th>releasing</th>\n",
       "      <th>rupee</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Document 1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Document 2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2000  500  black  curb  eradicating  help  introduced  modi  \\\n",
       "Document 1     1    1      1     0            1     1           0     0   \n",
       "Document 2     1    0      1     1            0     0           1     1   \n",
       "\n",
       "            money  new  note  pm  releasing  rupee  want  \n",
       "Document 1      1    1     1   0          1      1     0  \n",
       "Document 2      1    1     1   1          0      1     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "Document1 = df_train['question1_cleaned'][3]\n",
    "Document2 = df_train['question2_cleaned'][3]\n",
    "\n",
    "corpus = [Document1, Document2]\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names_out(),index=['Document 1','Document 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>500</th>\n",
       "      <th>black</th>\n",
       "      <th>curb</th>\n",
       "      <th>eradicating</th>\n",
       "      <th>help</th>\n",
       "      <th>introduced</th>\n",
       "      <th>modi</th>\n",
       "      <th>money</th>\n",
       "      <th>new</th>\n",
       "      <th>note</th>\n",
       "      <th>pm</th>\n",
       "      <th>releasing</th>\n",
       "      <th>rupee</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Document 1</th>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Document 2</th>\n",
       "      <td>0.250969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250969</td>\n",
       "      <td>0.352728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352728</td>\n",
       "      <td>0.352728</td>\n",
       "      <td>0.250969</td>\n",
       "      <td>0.250969</td>\n",
       "      <td>0.250969</td>\n",
       "      <td>0.352728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250969</td>\n",
       "      <td>0.352728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                2000       500     black      curb  eradicating      help  \\\n",
       "Document 1  0.268208  0.376957  0.268208  0.000000     0.376957  0.376957   \n",
       "Document 2  0.250969  0.000000  0.250969  0.352728     0.000000  0.000000   \n",
       "\n",
       "            introduced      modi     money       new      note        pm  \\\n",
       "Document 1    0.000000  0.000000  0.268208  0.268208  0.268208  0.000000   \n",
       "Document 2    0.352728  0.352728  0.250969  0.250969  0.250969  0.352728   \n",
       "\n",
       "            releasing     rupee      want  \n",
       "Document 1   0.376957  0.268208  0.000000  \n",
       "Document 2   0.000000  0.250969  0.352728  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "trsfm=vectorizer.fit_transform(corpus)\n",
    "pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names_out(),index=['Document 1','Document 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.40387178]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(trsfm[0:1], trsfm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
